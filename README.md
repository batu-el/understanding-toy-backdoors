# Understanding Toy Backdoors via Mechanistic Interpretability

[![Paper](https://img.shields.io/badge/Paper-007ACC?style=for-the-badge&labelColor=007ACC)](https://github.com/batu-el/understanding-toy-backdoors/blob/main/understanding-backdoors.pdf)
[![Drive Folder](https://img.shields.io/badge/Drive_Folder-007ACC?style=for-the-badge&labelColor=007ACC)](https://drive.google.com/drive/folders/1bYo5cmAbYJfbly7my4PkCFa21mJxQtdu?usp=sharing)
[![Course Page](https://img.shields.io/badge/Course_Page-007ACC?style=for-the-badge&labelColor=007ACC)](https://www.cl.cam.ac.uk/teaching/2324/R252/)

## Abstract
Backdoors and hidden harmful behaviour represent a severe risk to the safe deployment of deep neural networks. In this paper, we explore how a small Transformer model implements a toy backdoor behaviour. Our head attribution and activation patching experiments suggest that our model uses a single attention head to implement a simple backdoor. *Easy-to-run Colab notebooks for the experiments are available in the Google Drive Folder.*

Reference: https://github.com/TransformerLensOrg/TransformerLens

## Toy Backdoors
![Alt text](assets/transformerfigure.png)
![Alt text](assets/figure2.png)

## Training
![Alt text](assets/training.png)

## Results
![Alt text](assets/evaluationmod1.png)
![Alt text](assets/evaluation.png)
